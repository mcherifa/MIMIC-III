{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import time\n",
    "\n",
    "from pathlib import Path\n",
    "from sklearn import metrics\n",
    "import random\n",
    "from scipy import stats\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "import torchvision\n",
    "\n",
    "from datetime import datetime\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = Path(\"../../multi-task-romain/data/\")\n",
    "# PATH = Path(\"/data2/yinterian/multi-task-romain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'5min'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gap = \"5min\"\n",
    "gap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"data_train_{gap}.pickle\".format(gap=gap)\n",
    "with open(PATH/filename, 'rb') as f:\n",
    "    train = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"data_valid_{gap}.pickle\".format(gap=gap)\n",
    "with open(PATH/filename, 'rb') as f:\n",
    "    valid = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((59742, 14), (7086, 14))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_id_list = np.sort(np.unique(train.subject_id.values))\n",
    "id2index = {v: k+1 for k,v in enumerate(subject_id_list)}\n",
    "num_subjects = len(subject_id_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2295"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_subjects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_std_series(train):\n",
    "    ss = np.concatenate(train.series.values)\n",
    "    ss = ss.reshape(-1,5)\n",
    "    return ss.mean(axis=0), ss.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_std_static(train):\n",
    "    res = {}\n",
    "    for name in [\"age\", \"sapsii\", \"sofa\"]:\n",
    "        values = train[name].values\n",
    "        res[name] = (values.mean(), values.std())\n",
    "    res[\"series\"] = get_mean_std_series(train)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'age': (64, 15.073998327645949),\n",
       " 'sapsii': (33, 14.215114554630107),\n",
       " 'sofa': (4, 3.7687923741651197),\n",
       " 'series': (array([ 83.25271123,  93.7286662 , 120.81020051,  58.76277023,\n",
       "          78.52866913]),\n",
       "  array([16.10279665, 17.32261077, 21.2893833 , 12.28384779, 14.32805636]))}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_dict = get_mean_std_static(train)\n",
    "norm_dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTask(Dataset):\n",
    "    def __init__(self, df, norm_dict, id2index, k=20, train=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            df: dataframe with data\n",
    "            norm_dict: mean and std of all variables to normalize\n",
    "            \n",
    "        \"\"\"\n",
    "        self.norm_dict = norm_dict\n",
    "        self.df = df\n",
    "        self.names = [\"age\", \"sapsii\", \"sofa\"] ## needs normalization\n",
    "        self.names_binary = [\"gender\", \"amine\", \"sedation\", \"ventilation\"]\n",
    "        self.id2index = id2index\n",
    "        self.train = train\n",
    "        self.df_sample = self.pick_a_sample(k)\n",
    "            \n",
    "    def pick_a_sample(self, k=20):\n",
    "        \"\"\" Picks sample with the same number of observations per patient\"\"\"\n",
    "        if not self.train: # fix seed for validation and test\n",
    "            np.random.seed(3)\n",
    "        sample = self.df.groupby(\"subject_id\", group_keys=False).apply(lambda x: x.sample(min(len(x), k)))\n",
    "        sample = sample.copy()\n",
    "        if self.train:\n",
    "            self.subject_index = [self.id2index[subject_id] for subject_id in sample.subject_id.values]\n",
    "            self.random = np.random.choice(2, sample.shape[0], p=[0.1, 0.9])\n",
    "            self.subject_index = self.subject_index*self.random\n",
    "        return sample\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.df_sample.iloc[index,:]\n",
    "        x_series = (row.series - self.norm_dict[\"series\"][0])/self.norm_dict[\"series\"][1]\n",
    "        x_cont = [(row[name]-self.norm_dict[name][0])/self.norm_dict[name][1] for name in self.names]\n",
    "        x_binary = [row[name] for name in self.names_binary]\n",
    "        subject_index = 0\n",
    "        if self.train:\n",
    "            subject_index = self.subject_index[index]\n",
    "        x_cat = np.array([row[\"care_unit\"], subject_index])\n",
    "        x_cont = np.array(x_cont + x_binary)\n",
    "        return x_series, x_cont, x_cat, row[\"prediction_mean_HR\"], row[\"prediction_mean_MAP\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df_sample.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = MultiTask(train, norm_dict, id2index)\n",
    "valid_ds = MultiTask(valid, norm_dict, id2index, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.25134074,  0.35048607,  1.97233517,  3.2186356 ,  3.2643179 ],\n",
       "        [ 0.34449226,  0.36203168,  2.58296817,  3.7640673 ,  3.96922859],\n",
       "        [ 0.56805591,  0.35625887,  1.93475776,  3.09652402,  3.18754545],\n",
       "        [ 0.20165993,  0.36203168,  2.11794766,  3.25119868,  3.35504898],\n",
       "        [ 0.25755084,  0.36203168,  1.95354646,  3.03139785,  3.1596282 ],\n",
       "        [ 0.43764378,  0.31584926,  2.86949597,  3.8780381 ,  4.17162868],\n",
       "        [ 0.66120743,  0.23503003,  2.55008793,  3.62567418,  3.81568368],\n",
       "        [ 0.17681952,  0.24080284,  1.65292714,  2.70576698,  2.77576594],\n",
       "        [ 0.03398719,  0.30430366,  1.357005  ,  2.42898075,  2.46169683],\n",
       "        [ 0.01535688,  0.29275805,  1.1738151 ,  2.33129149,  2.30815193],\n",
       "        [-0.03432393,  0.29853085,  1.10335744,  2.23360223,  2.22440016],\n",
       "        [-0.10884514,  0.30430366,  1.00471673,  2.17661682,  2.13366908],\n",
       "        [-0.15231585,  0.21771163,  1.04699132,  2.2987284 ,  2.21044153],\n",
       "        [-0.12747545,  0.05607318,  0.93425907,  2.14405374,  2.07085526],\n",
       "        [-0.10884514,  0.09070999,  0.81682965,  2.03008293,  1.95918624],\n",
       "        [-0.15852596,  0.25812124,  0.826224  ,  2.08706834,  1.98012418],\n",
       "        [-0.17094616,  0.26966685,  1.04699132,  2.30686917,  2.22440016],\n",
       "        [-0.18957646,  0.31584926,  1.16442074,  2.42083997,  2.35002781],\n",
       "        [-0.22683707,  0.30430366,  1.12214615,  2.36385457,  2.28721398],\n",
       "        [-0.24546738,  0.30430366,  1.10335744,  2.33943226,  2.27325536],\n",
       "        [-0.22683707,  0.30430366,  1.18790663,  2.43712152,  2.37794506],\n",
       "        [-0.23925727,  0.30430366,  0.9295619 ,  2.17661682,  2.08481389],\n",
       "        [-0.20199667,  0.33316767,  1.01880826,  2.29058763,  2.19648291],\n",
       "        [-0.32619869,  0.31584926,  1.07517438,  2.31500994,  2.2453381 ],\n",
       "        [-0.3386189 ,  0.30430366,  0.99532237,  2.23360223,  2.15460702],\n",
       "        [-0.30135829,  0.30430366,  0.8684986 ,  2.14405374,  2.03595869],\n",
       "        [-0.28272798,  0.32162206,  0.87789295,  2.16033528,  2.04991732],\n",
       "        [-0.23925727,  0.31007646,  1.08456873,  2.39641766,  2.2941933 ],\n",
       "        [-0.344829  ,  0.36203168,  1.14093486,  2.42083997,  2.33606918],\n",
       "        [-0.31998859,  0.35625887,  1.08926591,  2.3557138 ,  2.25929673]]),\n",
       " array([-0.0663394 , -1.05521485, -0.79601095,  1.        ,  0.        ,\n",
       "         0.        ,  1.        ]),\n",
       " array([ 0, 88]),\n",
       " 83.28,\n",
       " 123.88)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1, x2, x3, y1, y2 = train_ds[1200]\n",
    "x1, x2, x3, y1, y2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(m, p): torch.save(m.state_dict(), p)\n",
    "    \n",
    "def load_model(m, p): m.load_state_dict(torch.load(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pearsonr_ci(x,y,alpha=0.05):\n",
    "    ''' calculate Pearson correlation along with the confidence interval using scipy and numpy\n",
    "    Parameters\n",
    "    ----------\n",
    "    x, y : iterable object such as a list or np.array\n",
    "      Input for correlation calculation\n",
    "    alpha : float\n",
    "      Significance level. 0.05 by default\n",
    "    Returns\n",
    "    -------\n",
    "    r : float\n",
    "      Pearson's correlation coefficient\n",
    "    pval : float\n",
    "      The corresponding p value\n",
    "    lo, hi : float\n",
    "      The lower and upper bound of confidence intervals\n",
    "    '''\n",
    "    r, p = stats.pearsonr(x,y)\n",
    "    r_z = np.arctanh(r)\n",
    "    se = 1/np.sqrt(x.size-3)\n",
    "    z = stats.norm.ppf(1-alpha/2)\n",
    "    lo_z, hi_z = r_z-z*se, r_z+z*se\n",
    "    lo, hi = np.tanh((lo_z, hi_z))\n",
    "    return r, lo, hi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_metrics(model, valid_dl, which_y=\"y1\"):\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    sum_loss = 0\n",
    "    y_hat = []\n",
    "    ys = []\n",
    "    for x_series, x_cont, x_cat, y1, y2 in valid_dl:\n",
    "        batch = y1.shape[0]\n",
    "        x_series = x_series.float()\n",
    "        x_cont = x_cont.float()\n",
    "        x_cat = x_cat.long()\n",
    "        y1 = y1.float()\n",
    "        y2 = y2.float()\n",
    "        out = model(x_series, x_cont, x_cat)\n",
    "        if which_y==\"y1\":\n",
    "            mse_loss = F.mse_loss(out,  y1.unsqueeze(-1))\n",
    "            ys.append(y1.view(-1).numpy())\n",
    "        else:\n",
    "            mse_loss = F.mse_loss(out, y2.unsqueeze(-1))\n",
    "            ys.append(y2.view(-1).numpy())\n",
    "        sum_loss += batch*(mse_loss.item())\n",
    "        total += batch\n",
    "        y_hat.append(out.view(-1).detach().numpy())\n",
    "    \n",
    "    y_hat = np.concatenate(y_hat)\n",
    "    ys = np.concatenate(ys)\n",
    "    #r2 = metrics.r2_score(ys, y_hat)\n",
    "    r2, lo, hi =  pearsonr_ci(ys, y_hat, alpha=0.05)\n",
    "    \n",
    "    return sum_loss/total, r2, lo, hi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epochs(model, train_ds, optimizer, lr=1e-3, epochs = 30, which_y=\"y1\"):\n",
    "    prev_val_r2 = 0\n",
    "    for i in range(epochs):\n",
    "        sum_loss = 0\n",
    "        total = 0\n",
    "        train_ds.pick_a_sample()\n",
    "        train_dl = DataLoader(train_ds, batch_size=5000, shuffle=True)\n",
    "        for x_series, x_cont, x_cat, y1, y2 in train_dl:\n",
    "            model.train()\n",
    "            x_series = x_series.float()\n",
    "            x_cont = x_cont.float()\n",
    "            x_cat = x_cat.long()\n",
    "            y1 = y1.float()\n",
    "            y2 = y2.float()\n",
    "            out = model(x_series, x_cont, x_cat)\n",
    "            if which_y==\"y1\":\n",
    "                loss = F.mse_loss(out, y1.unsqueeze(-1))\n",
    "            else:\n",
    "                loss = F.mse_loss(out, y2.unsqueeze(-1))\n",
    "            sum_loss += y1.shape[0] * loss.item()\n",
    "            \n",
    "            total += y1.shape[0]\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        if i % 1 == 0:\n",
    "            print(\"iteration : \", i)\n",
    "            val_loss, val_r2 , val_lo,  val_hi = val_metrics(model, valid_dl, which_y=which_y)\n",
    "            print(\"\\tTrain loss: {:.3f} \\n \\t valid loss: {:.3f} valid r2 {:.3f}[{:.3f}-{:.3f}]\".format(\n",
    "                sum_loss/total, val_loss, val_r2, val_lo, val_hi))\n",
    "        if val_r2 > prev_val_r2:\n",
    "            prev_val_r2 = val_r2\n",
    "            if val_r2 > 0.95 :\n",
    "                PATH = Path(\"../../multi-task-romain/2e_analyse/singletask/\")\n",
    "                filename = \"single_model_5min_\" + which_y\n",
    "                path = \"{0}/{1}_r2_{2:.0f}.pth\".format(PATH, filename, 100*val_r2) \n",
    "                save_model(model, path)\n",
    "                print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5000\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EventModel2(nn.Module):\n",
    "    def __init__(self, hidden_size=100):\n",
    "        super(EventModel2, self).__init__()\n",
    "        self.embedding1 = nn.Embedding(5, 1)\n",
    "        self.embedding2 = nn.Embedding(num_subjects+1, 5)\n",
    "        self.gru = nn.GRU(5, hidden_size, batch_first=True)\n",
    "        self.num = hidden_size + 1 + 5 + 7\n",
    "        self.linear1 = nn.Linear(self.num, self.num)\n",
    "        self.out = nn.Linear(self.num, 1)\n",
    "        self.bn1 = nn.BatchNorm1d(self.num)\n",
    "        \n",
    "    def forward(self, x_series, x_cont, x_cat):\n",
    "        _, ht = self.gru(x_series)\n",
    "        x_cat_1 = self.embedding1(x_cat[:,0])\n",
    "        x_cat_2 = self.embedding2(x_cat[:,1])\n",
    "        x = torch.cat((ht[-1], x_cat_1, x_cat_2, x_cont), 1)\n",
    "        x = self.bn1(F.relu(self.linear1(x)))\n",
    "        return self.out(x)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration :  0\n",
      "\tTrain loss: 6803.372 \n",
      " \t valid loss: 6147.004 valid r2 0.925[0.921-0.930]\n",
      "iteration :  1\n",
      "\tTrain loss: 4543.361 \n",
      " \t valid loss: 2223.938 valid r2 0.952[0.949-0.955]\n",
      "../../multi-task-romain/2e_analyse/singletask/single_model_5min_y1_r2_95.pth\n",
      "iteration :  2\n",
      "\tTrain loss: 1150.275 \n",
      " \t valid loss: 19.483 valid r2 0.977[0.976-0.979]\n",
      "../../multi-task-romain/2e_analyse/singletask/single_model_5min_y1_r2_98.pth\n",
      "iteration :  3\n",
      "\tTrain loss: 385.472 \n",
      " \t valid loss: 836.963 valid r2 0.981[0.980-0.982]\n",
      "../../multi-task-romain/2e_analyse/singletask/single_model_5min_y1_r2_98.pth\n",
      "iteration :  4\n",
      "\tTrain loss: 447.922 \n",
      " \t valid loss: 33.719 valid r2 0.982[0.981-0.983]\n",
      "../../multi-task-romain/2e_analyse/singletask/single_model_5min_y1_r2_98.pth\n",
      "iteration :  5\n",
      "\tTrain loss: 58.677 \n",
      " \t valid loss: 179.738 valid r2 0.984[0.983-0.985]\n",
      "../../multi-task-romain/2e_analyse/singletask/single_model_5min_y1_r2_98.pth\n",
      "iteration :  6\n",
      "\tTrain loss: 166.380 \n",
      " \t valid loss: 93.759 valid r2 0.984[0.983-0.985]\n",
      "../../multi-task-romain/2e_analyse/singletask/single_model_5min_y1_r2_98.pth\n",
      "iteration :  7\n",
      "\tTrain loss: 35.410 \n",
      " \t valid loss: 16.600 valid r2 0.985[0.984-0.986]\n",
      "../../multi-task-romain/2e_analyse/singletask/single_model_5min_y1_r2_98.pth\n",
      "iteration :  8\n",
      "\tTrain loss: 41.812 \n",
      " \t valid loss: 36.059 valid r2 0.985[0.984-0.986]\n",
      "../../multi-task-romain/2e_analyse/singletask/single_model_5min_y1_r2_99.pth\n",
      "iteration :  9\n",
      "\tTrain loss: 23.149 \n",
      " \t valid loss: 8.732 valid r2 0.985[0.984-0.986]\n",
      "iteration :  10\n",
      "\tTrain loss: 16.189 \n",
      " \t valid loss: 18.370 valid r2 0.986[0.985-0.987]\n",
      "../../multi-task-romain/2e_analyse/singletask/single_model_5min_y1_r2_99.pth\n",
      "iteration :  11\n",
      "\tTrain loss: 14.377 \n",
      " \t valid loss: 7.690 valid r2 0.986[0.985-0.987]\n",
      "../../multi-task-romain/2e_analyse/singletask/single_model_5min_y1_r2_99.pth\n",
      "iteration :  12\n",
      "\tTrain loss: 10.826 \n",
      " \t valid loss: 10.065 valid r2 0.986[0.985-0.987]\n",
      "../../multi-task-romain/2e_analyse/singletask/single_model_5min_y1_r2_99.pth\n",
      "iteration :  13\n",
      "\tTrain loss: 10.855 \n",
      " \t valid loss: 6.980 valid r2 0.986[0.985-0.986]\n",
      "iteration :  14\n",
      "\tTrain loss: 9.489 \n",
      " \t valid loss: 8.565 valid r2 0.987[0.986-0.987]\n",
      "../../multi-task-romain/2e_analyse/singletask/single_model_5min_y1_r2_99.pth\n"
     ]
    }
   ],
   "source": [
    "# model for mean_HR\n",
    "model = EventModel2()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.05, weight_decay=1e-5)\n",
    "train_epochs(model, train_ds, optimizer, epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration :  0\n",
      "\tTrain loss: 5942.173 \n",
      " \t valid loss: 6356.440 valid r2 0.920[0.915-0.925]\n",
      "iteration :  1\n",
      "\tTrain loss: 4035.195 \n",
      " \t valid loss: 1733.517 valid r2 0.727[0.711-0.742]\n",
      "iteration :  2\n",
      "\tTrain loss: 983.413 \n",
      " \t valid loss: 82.892 valid r2 0.921[0.916-0.926]\n",
      "iteration :  3\n",
      "\tTrain loss: 393.536 \n",
      " \t valid loss: 909.684 valid r2 0.950[0.947-0.954]\n",
      "../../multi-task-romain/2e_analyse/singletask/single_model_5min_y2_r2_95.pth\n",
      "iteration :  4\n",
      "\tTrain loss: 363.113 \n",
      " \t valid loss: 24.010 valid r2 0.959[0.956-0.961]\n",
      "../../multi-task-romain/2e_analyse/singletask/single_model_5min_y2_r2_96.pth\n",
      "iteration :  5\n",
      "\tTrain loss: 69.397 \n",
      " \t valid loss: 188.409 valid r2 0.960[0.957-0.962]\n",
      "../../multi-task-romain/2e_analyse/singletask/single_model_5min_y2_r2_96.pth\n",
      "iteration :  6\n",
      "\tTrain loss: 149.832 \n",
      " \t valid loss: 83.706 valid r2 0.966[0.964-0.968]\n",
      "../../multi-task-romain/2e_analyse/singletask/single_model_5min_y2_r2_97.pth\n",
      "iteration :  7\n",
      "\tTrain loss: 29.982 \n",
      " \t valid loss: 25.784 valid r2 0.963[0.960-0.965]\n",
      "iteration :  8\n",
      "\tTrain loss: 45.955 \n",
      " \t valid loss: 30.750 valid r2 0.969[0.967-0.971]\n",
      "../../multi-task-romain/2e_analyse/singletask/single_model_5min_y2_r2_97.pth\n",
      "iteration :  9\n",
      "\tTrain loss: 21.625 \n",
      " \t valid loss: 14.557 valid r2 0.970[0.968-0.972]\n",
      "../../multi-task-romain/2e_analyse/singletask/single_model_5min_y2_r2_97.pth\n"
     ]
    }
   ],
   "source": [
    "# model mean_MAP\n",
    "model = EventModel2()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.05, weight_decay=1e-5)\n",
    "train_epochs(model, train_ds, optimizer, epochs=10, which_y=\"y2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = Path(\"../../multi-task-romain/2e_analyse/singletask/\")\n",
    "path = PATH/\"single_model_5min_y1_r2_99.pth\"\n",
    "model = EventModel2()\n",
    "load_model(model, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8233, 14), (1597, 13))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = \"../../data/data_test_{gap}.pickle\".format(gap=gap)\n",
    "with open(PATH/filename, 'rb') as f:\n",
    "    test = pickle.load(f)\n",
    "    \n",
    "filename = \"../../data/data_validation_{gap}.pickle\".format(gap=gap)\n",
    "with open(PATH/filename, 'rb') as f:\n",
    "    test_larib = pickle.load(f)\n",
    "test_larib[\"care_unit\"] = 4\n",
    "test.shape, test_larib.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_y1_one_batch(model, dl):\n",
    "    for x_series, x_cont, x_cat, y1, y2 in dl:\n",
    "        x_series = x_series.float()\n",
    "        x_cont = x_cont.float()\n",
    "        x_cat = x_cat.long()\n",
    "        y1 = y1.float()\n",
    "        out1 = model(x_series, x_cont, x_cat)\n",
    "    return out1.detach().numpy(), y1.detach().numpy()\n",
    "\n",
    "class MultiTask_validation(Dataset):\n",
    "    def __init__(self, df, norm_dict, id2index, k=20, train=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            df: dataframe with data\n",
    "            norm_dict: mean and std of all variables to normalize\n",
    "            \n",
    "        \"\"\"\n",
    "        self.norm_dict = norm_dict\n",
    "        self.df = df\n",
    "        self.names = [\"age\", \"sapsii\", \"sofa\"] ## needs normalization\n",
    "        self.names_binary = [\"gender\", \"amine\", \"sedation\", \"ventilation\"]\n",
    "        self.id2index = id2index\n",
    "        self.train = train\n",
    "        self.df_sample = self.pick_a_sample(k)\n",
    "            \n",
    "    def pick_a_sample(self, k=20):\n",
    "        \"\"\" Picks sample with the same number of observations per patient\"\"\"\n",
    "        if not self.train: # fix seed for validation and test\n",
    "            np.random.seed(3)\n",
    "# We don't want the same number of period per patient\n",
    "        # sample = self.df.groupby(\"subject_id\", group_keys=False).apply(lambda x: x.sample(k, replace=True))\n",
    "        sample = self.df.copy()\n",
    "        if self.train:\n",
    "# 10 percent of the periods have a subject_index == 0\n",
    "            self.subject_index = [self.id2index[subject_id] for subject_id in sample.subject_id.values]\n",
    "            self.random = np.random.choice(2, sample.shape[0], p = [0.1, 0.9])\n",
    "            self.subject_index = self.subject_index*self.random\n",
    "        return sample\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.df_sample.iloc[index,:] \n",
    "        x_series = (row.series - self.norm_dict[\"series\"][0])/self.norm_dict[\"series\"][1]\n",
    "        x_cont = [(row[name]-self.norm_dict[name][0])/self.norm_dict[name][1] for name in self.names]\n",
    "        x_binary = [row[name] for name in self.names_binary]\n",
    "        subject_index = 0\n",
    "        if self.train:\n",
    "            subject_index = self.subject_index[index]\n",
    "        x_cat = np.array([row[\"care_unit\"], subject_index])\n",
    "        x_cont = np.array(x_cont + x_binary)\n",
    "        return x_series, x_cont, x_cat, row[\"prediction_mean_HR\"], row[\"prediction_mean_MAP\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df_sample.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = MultiTask_validation(test, norm_dict, id2index, train=False)\n",
    "test_larib_ds = MultiTask_validation(test_larib, norm_dict, id2index, train = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dl = DataLoader(test_ds, batch_size=8233)\n",
    "test_larib_dl = DataLoader(test_larib_ds, batch_size=1597)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11.105539321899414, 0.9828773, 0.982127995589721, 0.9835954819543968)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_metrics(model, test_dl, which_y=\"y1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40.03071594238281, 0.938873, 0.9327788412510054, 0.9444305544122914)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_metrics(model, test_larib_dl, which_y=\"y1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HR\n",
    "out1, y1 = predict_y1_one_batch(model, test_dl)\n",
    "y1 = np.reshape(y1, (-1,1))\n",
    "arr_hr = np.concatenate((out1, y1) , axis=1)\n",
    "pd.DataFrame(arr_hr).to_csv(\"/home/menyssa/Recherche/Mimic-III-Yannet/resultats/2e_analyse/intern_single_obs_pred_HR_5.csv\")\n",
    "\n",
    "\n",
    "out1, y1 = predict_y1_one_batch(model, test_larib_dl)\n",
    "y1 = np.reshape(y1, (-1,1))\n",
    "arr_hr = np.concatenate((out1, y1) , axis=1)\n",
    "pd.DataFrame(arr_hr).to_csv(\"/home/menyssa/Recherche/Mimic-III-Yannet/resultats/2e_analyse/larib_single_obs_pred_HR_5.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = PATH/\"single_model_5min_y2_r2_97.pth\"\n",
    "load_model(model, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_y2_one_batch(model, dl):\n",
    "    for x_series, x_cont, x_cat, y1, y2 in dl:\n",
    "        x_series = x_series.float()\n",
    "        x_cont = x_cont.float()\n",
    "        x_cat = x_cat.long()\n",
    "        y2 = y2.float()\n",
    "        out2 = model(x_series, x_cont, x_cat)\n",
    "    return out2.detach().numpy(), y2.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17.737394332885742, 0.9677076, 0.9663056756152778, 0.9690520620987308)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_metrics(model, test_dl, which_y=\"y2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(59.321895599365234, 0.9233717, 0.9157992008776028, 0.9302879087778957)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_metrics(model, test_larib_dl, which_y=\"y2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "out2, y2 = predict_y2_one_batch(model, test_dl)\n",
    "y2 = np.reshape(y2, (-1,1))\n",
    "arr_map = np.concatenate((out2, y2) , axis=1)\n",
    "pd.DataFrame(arr_map).to_csv(\"/home/menyssa/Recherche/Mimic-III-Yannet/resultats/2e_analyse/intern_single_obs_pred_MAP_5.csv\")\n",
    "\n",
    "out2, y2 = predict_y2_one_batch(model, test_larib_dl)\n",
    "y2 = np.reshape(y2, (-1,1))\n",
    "arr_map = np.concatenate((out2, y2) , axis=1)\n",
    "pd.DataFrame(arr_map).to_csv(\"/home/menyssa/Recherche/Mimic-III-Yannet/resultats/2e_analyse/larib_single_obs_pred_MAP_5.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## looking at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filename = \"data_train_{gap}.pickle\".format(gap=\"5min\")\n",
    "#with open(PATH/filename, 'rb') as f:\n",
    "#    train5 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filename = \"data_train_{gap}.pickle\".format(gap=\"10min\")\n",
    "#with open(PATH/filename, 'rb') as f:\n",
    "#    train10 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cols = [\"subject_id\", \"key\", \"prediction_mean_HR\", \"prediction_mean_MAP\"]\n",
    "#train5_s = train5.loc[:, cols]\n",
    "#train10_s = train10.loc[:, cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train5_s.iloc[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train10_s.iloc[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

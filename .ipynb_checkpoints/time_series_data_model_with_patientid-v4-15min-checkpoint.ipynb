{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import time\n",
    "\n",
    "from pathlib import Path\n",
    "from sklearn import metrics\n",
    "import random\n",
    "from scipy import stats\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "import torchvision\n",
    "\n",
    "from datetime import datetime\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = Path(\"../../multi-task-romain/data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'15min'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gap = \"15min\"\n",
    "gap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"data_train_{gap}.pickle\".format(gap=gap)\n",
    "with open(PATH/filename, 'rb') as f:\n",
    "    train = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"data_valid_{gap}.pickle\".format(gap=gap)\n",
    "with open(PATH/filename, 'rb') as f:\n",
    "    valid = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>key</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>sapsii</th>\n",
       "      <th>sofa</th>\n",
       "      <th>care_unit</th>\n",
       "      <th>amine</th>\n",
       "      <th>sedation</th>\n",
       "      <th>ventilation</th>\n",
       "      <th>prediction_mean_HR</th>\n",
       "      <th>prediction_mean_MAP</th>\n",
       "      <th>series</th>\n",
       "      <th>group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10013</td>\n",
       "      <td>10013_15</td>\n",
       "      <td>1</td>\n",
       "      <td>87</td>\n",
       "      <td>49</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>104.56</td>\n",
       "      <td>64.26</td>\n",
       "      <td>[[95.8, 93.0, 103.8, 34.8, 54.8], [99.2, 93.9,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10013</td>\n",
       "      <td>10013_16</td>\n",
       "      <td>1</td>\n",
       "      <td>87</td>\n",
       "      <td>49</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>102.08</td>\n",
       "      <td>68.28</td>\n",
       "      <td>[[106.7, 95.3, 115.4, 41.5, 65.0], [107.4, 95....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10013</td>\n",
       "      <td>10013_19</td>\n",
       "      <td>1</td>\n",
       "      <td>87</td>\n",
       "      <td>49</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>100.22</td>\n",
       "      <td>65.62</td>\n",
       "      <td>[[95.4, 92.0, 133.3, 56.1, 83.0], [98.2, 91.9,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10013</td>\n",
       "      <td>10013_20</td>\n",
       "      <td>1</td>\n",
       "      <td>87</td>\n",
       "      <td>49</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>106.44</td>\n",
       "      <td>57.74</td>\n",
       "      <td>[[100.0, 94.6, 108.4, 39.0, 63.1], [102.3, 94....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10013</td>\n",
       "      <td>10013_22</td>\n",
       "      <td>1</td>\n",
       "      <td>87</td>\n",
       "      <td>49</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>93.88</td>\n",
       "      <td>56.74</td>\n",
       "      <td>[[99.8, 91.6, 89.9, 37.7, 57.0], [99.3, 91.1, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subject_id       key gender age sapsii sofa  care_unit amine sedation  \\\n",
       "0       10013  10013_15      1  87     49    7          2     1        0   \n",
       "1       10013  10013_16      1  87     49    7          2     1        0   \n",
       "2       10013  10013_19      1  87     49    7          2     1        0   \n",
       "3       10013  10013_20      1  87     49    7          2     1        0   \n",
       "4       10013  10013_22      1  87     49    7          2     1        0   \n",
       "\n",
       "  ventilation prediction_mean_HR prediction_mean_MAP  \\\n",
       "0           1             104.56               64.26   \n",
       "1           1             102.08               68.28   \n",
       "2           1             100.22               65.62   \n",
       "3           1             106.44               57.74   \n",
       "4           1              93.88               56.74   \n",
       "\n",
       "                                              series  group  \n",
       "0  [[95.8, 93.0, 103.8, 34.8, 54.8], [99.2, 93.9,...      0  \n",
       "1  [[106.7, 95.3, 115.4, 41.5, 65.0], [107.4, 95....      0  \n",
       "2  [[95.4, 92.0, 133.3, 56.1, 83.0], [98.2, 91.9,...      0  \n",
       "3  [[100.0, 94.6, 108.4, 39.0, 63.1], [102.3, 94....      0  \n",
       "4  [[99.8, 91.6, 89.9, 37.7, 57.0], [99.3, 91.1, ...      0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>key</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>sapsii</th>\n",
       "      <th>sofa</th>\n",
       "      <th>care_unit</th>\n",
       "      <th>amine</th>\n",
       "      <th>sedation</th>\n",
       "      <th>ventilation</th>\n",
       "      <th>prediction_mean_HR</th>\n",
       "      <th>prediction_mean_MAP</th>\n",
       "      <th>series</th>\n",
       "      <th>group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>11018</td>\n",
       "      <td>11018_23</td>\n",
       "      <td>0</td>\n",
       "      <td>67</td>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>76.04</td>\n",
       "      <td>69.9867</td>\n",
       "      <td>[[79.0, 96.0, 98.0, 60.0, 71.0], [80.0, 95.0, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001</th>\n",
       "      <td>11018</td>\n",
       "      <td>11018_24</td>\n",
       "      <td>0</td>\n",
       "      <td>67</td>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>75.24</td>\n",
       "      <td>74.6167</td>\n",
       "      <td>[[77.0, 97.0, 99.0, 59.0, 72.0], [75.0, 97.0, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1002</th>\n",
       "      <td>11018</td>\n",
       "      <td>11018_26</td>\n",
       "      <td>0</td>\n",
       "      <td>67</td>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>81.42</td>\n",
       "      <td>80.19</td>\n",
       "      <td>[[81.0, 96.0, 109.0, 66.0, 80.0], [80.0, 96.0,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1003</th>\n",
       "      <td>11018</td>\n",
       "      <td>11018_27</td>\n",
       "      <td>0</td>\n",
       "      <td>67</td>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>81.3867</td>\n",
       "      <td>83.5367</td>\n",
       "      <td>[[80.0, 94.0, 113.0, 65.0, 80.0], [83.0, 94.0,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1004</th>\n",
       "      <td>11018</td>\n",
       "      <td>11018_28</td>\n",
       "      <td>0</td>\n",
       "      <td>67</td>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>79.7367</td>\n",
       "      <td>82.6133</td>\n",
       "      <td>[[84.0, 95.0, 114.0, 67.0, 81.0], [81.0, 94.0,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      subject_id       key gender age sapsii sofa  care_unit amine sedation  \\\n",
       "1000       11018  11018_23      0  67     47    1          0     0        0   \n",
       "1001       11018  11018_24      0  67     47    1          0     0        0   \n",
       "1002       11018  11018_26      0  67     47    1          0     0        0   \n",
       "1003       11018  11018_27      0  67     47    1          0     0        0   \n",
       "1004       11018  11018_28      0  67     47    1          0     0        0   \n",
       "\n",
       "     ventilation prediction_mean_HR prediction_mean_MAP  \\\n",
       "1000           0              76.04             69.9867   \n",
       "1001           0              75.24             74.6167   \n",
       "1002           0              81.42               80.19   \n",
       "1003           0            81.3867             83.5367   \n",
       "1004           0            79.7367             82.6133   \n",
       "\n",
       "                                                 series  group  \n",
       "1000  [[79.0, 96.0, 98.0, 60.0, 71.0], [80.0, 95.0, ...      1  \n",
       "1001  [[77.0, 97.0, 99.0, 59.0, 72.0], [75.0, 97.0, ...      1  \n",
       "1002  [[81.0, 96.0, 109.0, 66.0, 80.0], [80.0, 96.0,...      1  \n",
       "1003  [[80.0, 94.0, 113.0, 65.0, 80.0], [83.0, 94.0,...      1  \n",
       "1004  [[84.0, 95.0, 114.0, 67.0, 81.0], [81.0, 94.0,...      1  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((42830, 14), (5069, 14))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_id_list = np.sort(np.unique(train.subject_id.values))\n",
    "id2index = {v: k+1 for k,v in enumerate(subject_id_list)}\n",
    "num_subjects = len(subject_id_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2170"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of subjects in training\n",
    "num_subjects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_std_series(train):\n",
    "    ss = np.concatenate(train.series.values)\n",
    "    ss = ss.reshape(-1,5)\n",
    "    return ss.mean(axis=0), ss.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_std_static(train):\n",
    "    res = {}\n",
    "    for name in [\"age\", \"sapsii\", \"sofa\"]:\n",
    "        values = train[name].values\n",
    "        res[name] = (values.mean(), values.std())\n",
    "    res[\"series\"] = get_mean_std_series(train)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'age': (64, 15.087455295966063),\n",
       " 'sapsii': (33, 14.265492481117855),\n",
       " 'sofa': (4, 3.7831641172054082),\n",
       " 'series': (array([ 83.19453341,  93.64397046, 121.07613603,  58.73969887,\n",
       "          78.6694367 ]),\n",
       "  array([16.08727268, 17.53684697, 21.3399693 , 12.26982071, 14.36323955]))}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_dict = get_mean_std_static(train)\n",
    "norm_dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTask(Dataset):\n",
    "    def __init__(self, df, norm_dict, id2index, k=20, train=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            df: dataframe with data\n",
    "            norm_dict: mean and std of all variables to normalize\n",
    "            \n",
    "        \"\"\"\n",
    "        self.norm_dict = norm_dict\n",
    "        self.df = df\n",
    "        self.names = [\"age\", \"sapsii\", \"sofa\"] ## needs normalization\n",
    "        self.names_binary = [\"gender\", \"amine\", \"sedation\", \"ventilation\"]\n",
    "        self.id2index = id2index\n",
    "        self.train = train\n",
    "        self.df_sample = self.pick_a_sample(k)\n",
    "            \n",
    "    def pick_a_sample(self, k=20):\n",
    "        \"\"\" Picks sample with the same number of observations per patient\"\"\"\n",
    "        if not self.train: # fix seed for validation and test\n",
    "            np.random.seed(3)\n",
    "        sample = self.df.groupby(\"subject_id\", group_keys=False).apply(lambda x: x.sample(min(len(x), k)))\n",
    "        sample = sample.copy()\n",
    "        if self.train:\n",
    "            self.subject_index = [self.id2index[subject_id] for subject_id in sample.subject_id.values]\n",
    "            self.random = np.random.choice(2, sample.shape[0], p=[0.1, 0.9])\n",
    "            self.subject_index = self.subject_index*self.random\n",
    "        return sample\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.df_sample.iloc[index,:]\n",
    "        x_series = (row.series - self.norm_dict[\"series\"][0])/self.norm_dict[\"series\"][1]\n",
    "        x_cont = [(row[name]-self.norm_dict[name][0])/self.norm_dict[name][1] for name in self.names]\n",
    "        x_binary = [row[name] for name in self.names_binary]\n",
    "        subject_index = 0\n",
    "        if self.train:\n",
    "            subject_index = self.subject_index[index]\n",
    "        x_cat = np.array([row[\"care_unit\"], subject_index])\n",
    "        x_cont = np.array(x_cont + x_binary)\n",
    "        return x_series, x_cont, x_cat, row[\"prediction_mean_HR\"], row[\"prediction_mean_MAP\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df_sample.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = MultiTask(train, norm_dict, id2index, train = True)\n",
    "valid_ds = MultiTask(valid, norm_dict, id2index, train = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24690 2924\n"
     ]
    }
   ],
   "source": [
    "print(len(train_ds), len(valid_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.59086874,  0.13434739, -0.39719532, -0.4514898 , -0.38079409],\n",
       "        [ 0.62194921,  0.13434739, -0.3503349 , -0.40258933, -0.32509635],\n",
       "        [ 0.45411467,  0.12864511, -0.41125345, -0.51669042, -0.42952961],\n",
       "        [ 0.80843203,  0.13434739, -0.40656741, -0.41073941, -0.345983  ],\n",
       "        [ 0.45411467,  0.13434739, -0.31284656, -0.47594003, -0.37383187],\n",
       "        [-0.70829491,  0.13434739, -0.08323049, -0.7530427 , -0.49218957],\n",
       "        [-0.34154536,  0.11724055,  0.02923453, -0.76934285, -0.45041626],\n",
       "        [-1.40449745,  0.11153827,  0.21199018, -1.06274567, -0.53396288],\n",
       "        [ 0.2179031 ,  0.08872915,  0.00580432, -0.57374097, -0.31813413],\n",
       "        [ 0.69654234,  0.10013371, -0.05511423, -0.19068729, -0.06053208],\n",
       "        [ 0.51005952,  0.10583599, -0.1066607 , -0.25588792, -0.11622982],\n",
       "        [ 0.81464813,  0.12864511, -0.14414904, -0.25588792, -0.14407869],\n",
       "        [ 0.82708032,  0.12864511, -0.25192801, -0.27218807, -0.20673865],\n",
       "        [ 0.8395125 ,  0.13434739, -0.30347448, -0.36183894, -0.27636082],\n",
       "        [ 0.52249171,  0.11724055, -0.34096282, -0.4514898 , -0.33902078],\n",
       "        [ 0.65302968,  0.13434739, -0.20975363, -0.32923862, -0.21370086],\n",
       "        [ 0.72140672,  0.13434739, -0.17695133, -0.32108854, -0.17888978],\n",
       "        [ 0.67789406,  0.13434739, -0.18163738, -0.3373887 , -0.19977643],\n",
       "        [ 0.65302968,  0.12864511, -0.3503349 , -0.38628917, -0.29724748],\n",
       "        [ 0.83329641,  0.13434739, -0.18632342, -0.27218807, -0.1510409 ],\n",
       "        [ 0.65924578,  0.13434739, -0.28004427, -0.36998901, -0.25547417],\n",
       "        [ 0.90788954,  0.13434739, -0.33627677, -0.31293847, -0.24851195],\n",
       "        [ 0.70897453,  0.12864511, -0.29878843, -0.31293847, -0.24154973],\n",
       "        [ 0.65924578,  0.11153827, -0.31284656, -0.34553878, -0.26939861],\n",
       "        [ 0.65924578,  0.11153827, -0.26598614, -0.36183894, -0.23458752],\n",
       "        [ 0.80221594,  0.12864511, -0.32221865, -0.35368886, -0.26243639],\n",
       "        [ 0.93897001,  0.12864511, -0.36439303, -0.28033815, -0.24154973],\n",
       "        [ 0.83329641,  0.11724055, -0.37376511, -0.32108854, -0.27636082],\n",
       "        [ 0.96383439,  0.09443143, -0.34096282, -0.264038  , -0.20673865],\n",
       "        [ 0.82086422,  0.11153827, -0.35970699, -0.30478839, -0.26939861]]),\n",
       " array([ 0.99420344, -0.56079382,  0.        ,  1.        ,  0.        ,\n",
       "         0.        ,  0.        ]),\n",
       " array([ 2, 99]),\n",
       " 90.88,\n",
       " 79.8)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1, x2, x3, y1, y2 = train_ds[1200]\n",
    "x1, x2, x3, y1, y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "care_unit\n",
       "0     6248\n",
       "1    14426\n",
       "2    10244\n",
       "3     8045\n",
       "4     3867\n",
       "dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tab = train.groupby(['care_unit']).size()\n",
    "tab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(m, p): torch.save(m.state_dict(), p)\n",
    "    \n",
    "def load_model(m, p): m.load_state_dict(torch.load(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pearsonr_ci(x,y,alpha=0.05):\n",
    "    ''' calculate Pearson correlation along with the confidence interval using scipy and numpy\n",
    "    Parameters\n",
    "    ----------\n",
    "    x, y : iterable object such as a list or np.array\n",
    "      Input for correlation calculation\n",
    "    alpha : float\n",
    "      Significance level. 0.05 by default\n",
    "    Returns\n",
    "    -------\n",
    "    r : float\n",
    "      Pearson's correlation coefficient\n",
    "    pval : float\n",
    "      The corresponding p value\n",
    "    lo, hi : float\n",
    "      The lower and upper bound of confidence intervals\n",
    "    '''\n",
    "    r, p = stats.pearsonr(x,y)\n",
    "    r_z = np.arctanh(r)\n",
    "    se = 1/np.sqrt(x.size-3)\n",
    "    z = stats.norm.ppf(1-alpha/2)\n",
    "    lo_z, hi_z = r_z-z*se, r_z+z*se\n",
    "    lo, hi = np.tanh((lo_z, hi_z))\n",
    "    return r, p, lo, hi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EventModel1(nn.Module):\n",
    "    def __init__(self, hidden_size=100):\n",
    "        super(EventModel1, self).__init__()\n",
    "        self.embedding1 = nn.Embedding(5, 1)\n",
    "        self.embedding2 = nn.Embedding(num_subjects+1, 5)\n",
    "        self.gru = nn.GRU(5, hidden_size, batch_first=True)\n",
    "        self.num = hidden_size + 10 + 1 + 5\n",
    "        self.linear1 = nn.Linear(7, 10)\n",
    "        self.out1 = nn.Linear(self.num, 1)\n",
    "        self.out2 = nn.Linear(self.num, 1)\n",
    "        self.bn1 = nn.BatchNorm1d(10)\n",
    "\n",
    "    def forward(self, x_series, x_cont, x_cat):\n",
    "        _, ht = self.gru(x_series)\n",
    "        x_cat_1 = self.embedding1(x_cat[:,0])\n",
    "        x_cat_2 = self.embedding2(x_cat[:,1])\n",
    "        x_cont = self.bn1(F.relu(self.linear1(x_cont))) \n",
    "        x = torch.cat((ht[-1], x_cat_1, x_cat_2, x_cont), 1)\n",
    "        return self.out1(x), self.out2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_metrics(model, valid_dl):\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    sum_loss = 0\n",
    "    y_hat1 = []\n",
    "    ys1 = []\n",
    "    y_hat2 = []\n",
    "    ys2 = []\n",
    "    for x_series, x_cont, x_cat, y1, y2 in valid_dl:\n",
    "        batch = y1.shape[0]\n",
    "        x_series = x_series.float()\n",
    "        x_cont = x_cont.float()\n",
    "        x_cat = x_cat.long()\n",
    "        y1 = y1.float()\n",
    "        y2 = y2.float()\n",
    "        out1, out2 = model(x_series, x_cont, x_cat)\n",
    "        mse_loss1 = F.mse_loss(out1, y1.unsqueeze(-1))\n",
    "        mse_loss2 = F.mse_loss(out2, y2.unsqueeze(-1))\n",
    "        loss = mse_loss1 + mse_loss2\n",
    "        sum_loss += batch*(loss.item())\n",
    "        total += batch\n",
    "        y_hat1.append(out1.view(-1).detach().numpy())\n",
    "        ys1.append(y1.view(-1).numpy())\n",
    "        y_hat2.append(out2.view(-1).detach().numpy())\n",
    "        ys2.append(y2.view(-1).numpy())\n",
    "    \n",
    "    y_hat1 = np.concatenate(y_hat1)\n",
    "    y_hat2 = np.concatenate(y_hat2)\n",
    "    ys1 = np.concatenate(ys1)\n",
    "    ys2 = np.concatenate(ys2)\n",
    "    r2_1, p, lo2_1, hi2_1 =  pearsonr_ci(ys1, y_hat1, alpha=0.05)\n",
    "    r2_2, p, lo2_2, hi2_2 =  pearsonr_ci(ys2, y_hat2, alpha=0.05)\n",
    "    #r2_1 = metrics.r2_score(ys1, y_hat1)\n",
    "    #r2_2 = metrics.r2_score(ys2, y_hat2)\n",
    "    #return sum_loss/total,\n",
    "    return sum_loss/total,  r2_1,lo2_1, hi2_1, r2_2, lo2_2, hi2_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epochs(model, train_ds, optimizer, filename, lr=1e-3, epochs = 30):\n",
    "    t = time.process_time()\n",
    "    prev_val_r2 = 0\n",
    "    for i in range(epochs):\n",
    "        sum_loss1 = 0\n",
    "        sum_loss2 = 0\n",
    "        total = 0\n",
    "        train_ds.pick_a_sample()\n",
    "        train_dl = DataLoader(train_ds, batch_size=5000, shuffle=True)\n",
    "        valid_dl = DataLoader(valid_ds, batch_size=batch_size)\n",
    "        for x_series, x_cont, x_cat, y1, y2 in train_dl:\n",
    "            model.train()\n",
    "            x_series = x_series.float()\n",
    "            x_cont = x_cont.float()\n",
    "            x_cat = x_cat.long()\n",
    "            y1 = y1.float()\n",
    "            y2 = y2.float()\n",
    "            out1, out2 = model(x_series, x_cont, x_cat)\n",
    "            mse_loss1 = F.mse_loss(out1, y1.unsqueeze(-1))\n",
    "            mse_loss2 = F.mse_loss(out2, y2.unsqueeze(-1))\n",
    "            loss = mse_loss1 + mse_loss2\n",
    "            sum_loss1 += len(y1) * mse_loss1.item()\n",
    "            sum_loss2 += len(y1) * mse_loss2.item()\n",
    "            total += len(y1)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        if i % 1 == 0:\n",
    "            print(\"iteration : \", i)\n",
    "            # val_loss, val_r2_1, val_r2_2 = val_metrics(model, valid_dl)\n",
    "            #train_loss, train_r2_1,  train_lo2_1,  train_hi2_1, train_r2_2,  train_lo2_2,  train_hi2_2  = val_metrics(model, train_dl)\n",
    "            #print(\"\\tTrain loss (after): {:.3f}\".format(train_loss)) \n",
    "            print(\"\\tTrain loss: {:.3f} {:.3f}\".format(sum_loss1/total, sum_loss2/total))             \n",
    "            val_loss, val_r2_1,  val_lo2_1,  val_hi2_1, val_r2_2,  val_lo2_2,  val_hi2_2  = val_metrics(model, valid_dl)\n",
    "            print(\"\\tValid loss: {:.3f} \\n valid r2 hr {:.3f}[{:.3f}-{:.3f}] valid r2 map {:.3f}[{:.3f}-{:.3f}]\".format(\n",
    "               val_loss, val_r2_1,  val_lo2_1,  val_hi2_1, val_r2_2,  val_lo2_2,  val_hi2_2))\n",
    "            \n",
    "        if val_r2_1 > prev_val_r2:\n",
    "            prev_val_r2 = val_r2_1\n",
    "            if val_r2_1 > 0.95:\n",
    "                PATH = Path(\"../../multi-task-romain/2e_analyse/multitask/\")\n",
    "                path = \"{0}/{1}_15min_r2_{2:.0f}_{3:.0f}.pth\".format(PATH, filename, 100*val_r2_1, 100*val_r2_2) \n",
    "                save_model(model, path)\n",
    "                print(path)\n",
    "    elapsed_time = time.process_time() - t\n",
    "    print('time consuming: ', elapsed_time, 'secs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5000\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EventModel1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13815.126953125,\n",
       " -0.5840295,\n",
       " -0.6074189451677094,\n",
       " -0.5596283483138074,\n",
       " -0.3275749,\n",
       " -0.3595541682054588,\n",
       " -0.29482706536541065)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_metrics(model, valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration :  0\n",
      "\tTrain loss: 6282.982 5556.228\n",
      "\tValid loss: 8821.104 \n",
      " valid r2 hr 0.531[0.504-0.556] valid r2 map 0.615[0.592-0.637]\n",
      "iteration :  1\n",
      "\tTrain loss: 3754.901 3301.039\n",
      "\tValid loss: 3914.982 \n",
      " valid r2 hr 0.411[0.380-0.441] valid r2 map 0.296[0.263-0.329]\n",
      "iteration :  2\n",
      "\tTrain loss: 1555.605 1350.882\n",
      "\tValid loss: 870.933 \n",
      " valid r2 hr 0.361[0.329-0.392] valid r2 map 0.240[0.206-0.274]\n",
      "iteration :  3\n",
      "\tTrain loss: 350.322 310.819\n",
      "\tValid loss: 504.907 \n",
      " valid r2 hr 0.554[0.528-0.578] valid r2 map 0.313[0.280-0.346]\n",
      "iteration :  4\n",
      "\tTrain loss: 224.835 238.055\n",
      "\tValid loss: 991.274 \n",
      " valid r2 hr 0.797[0.783-0.810] valid r2 map 0.670[0.650-0.690]\n",
      "iteration :  5\n",
      "\tTrain loss: 246.710 237.405\n",
      "\tValid loss: 650.106 \n",
      " valid r2 hr 0.838[0.827-0.849] valid r2 map 0.804[0.791-0.816]\n",
      "iteration :  6\n",
      "\tTrain loss: 127.079 105.118\n",
      "\tValid loss: 186.934 \n",
      " valid r2 hr 0.845[0.834-0.855] valid r2 map 0.836[0.825-0.847]\n",
      "iteration :  7\n",
      "\tTrain loss: 68.241 60.906\n",
      "\tValid loss: 112.097 \n",
      " valid r2 hr 0.888[0.880-0.896] valid r2 map 0.847[0.836-0.857]\n",
      "iteration :  8\n",
      "\tTrain loss: 64.637 64.222\n",
      "\tValid loss: 100.856 \n",
      " valid r2 hr 0.898[0.891-0.905] valid r2 map 0.857[0.847-0.866]\n",
      "iteration :  9\n",
      "\tTrain loss: 52.017 52.993\n",
      "\tValid loss: 104.140 \n",
      " valid r2 hr 0.894[0.887-0.901] valid r2 map 0.862[0.853-0.871]\n",
      "time consuming:  245.743006867 secs\n",
      "iteration :  0\n",
      "\tTrain loss: 42.941 46.030\n",
      "\tValid loss: 91.814 \n",
      " valid r2 hr 0.916[0.910-0.922] valid r2 map 0.873[0.864-0.881]\n",
      "iteration :  1\n",
      "\tTrain loss: 34.268 38.883\n",
      "\tValid loss: 81.431 \n",
      " valid r2 hr 0.922[0.916-0.927] valid r2 map 0.886[0.878-0.894]\n",
      "iteration :  2\n",
      "\tTrain loss: 29.313 34.698\n",
      "\tValid loss: 69.793 \n",
      " valid r2 hr 0.937[0.933-0.941] valid r2 map 0.900[0.893-0.907]\n",
      "iteration :  3\n",
      "\tTrain loss: 24.163 31.331\n",
      "\tValid loss: 61.831 \n",
      " valid r2 hr 0.946[0.942-0.949] valid r2 map 0.908[0.902-0.914]\n",
      "iteration :  4\n",
      "\tTrain loss: 21.248 28.050\n",
      "\tValid loss: 54.358 \n",
      " valid r2 hr 0.952[0.948-0.955] valid r2 map 0.920[0.914-0.925]\n",
      "../../multi-task-romain/2e_analyse/multitask/multi_model_15min_r2_95_92.pth\n",
      "iteration :  5\n",
      "\tTrain loss: 18.914 25.803\n",
      "\tValid loss: 50.742 \n",
      " valid r2 hr 0.956[0.953-0.959] valid r2 map 0.927[0.922-0.932]\n",
      "../../multi-task-romain/2e_analyse/multitask/multi_model_15min_r2_96_93.pth\n",
      "iteration :  6\n",
      "\tTrain loss: 17.390 23.694\n",
      "\tValid loss: 47.058 \n",
      " valid r2 hr 0.959[0.956-0.961] valid r2 map 0.932[0.927-0.936]\n",
      "../../multi-task-romain/2e_analyse/multitask/multi_model_15min_r2_96_93.pth\n",
      "iteration :  7\n",
      "\tTrain loss: 16.244 22.412\n",
      "\tValid loss: 44.433 \n",
      " valid r2 hr 0.962[0.959-0.965] valid r2 map 0.935[0.930-0.940]\n",
      "../../multi-task-romain/2e_analyse/multitask/multi_model_15min_r2_96_94.pth\n",
      "iteration :  8\n",
      "\tTrain loss: 15.972 21.420\n",
      "\tValid loss: 42.416 \n",
      " valid r2 hr 0.964[0.961-0.966] valid r2 map 0.939[0.934-0.943]\n",
      "../../multi-task-romain/2e_analyse/multitask/multi_model_15min_r2_96_94.pth\n",
      "iteration :  9\n",
      "\tTrain loss: 14.963 20.642\n",
      "\tValid loss: 40.963 \n",
      " valid r2 hr 0.964[0.962-0.967] valid r2 map 0.940[0.935-0.944]\n",
      "../../multi-task-romain/2e_analyse/multitask/multi_model_15min_r2_96_94.pth\n",
      "iteration :  10\n",
      "\tTrain loss: 14.535 20.300\n",
      "\tValid loss: 39.435 \n",
      " valid r2 hr 0.965[0.963-0.968] valid r2 map 0.941[0.937-0.945]\n",
      "../../multi-task-romain/2e_analyse/multitask/multi_model_15min_r2_97_94.pth\n",
      "iteration :  11\n",
      "\tTrain loss: 14.168 19.970\n",
      "\tValid loss: 37.648 \n",
      " valid r2 hr 0.967[0.964-0.969] valid r2 map 0.943[0.939-0.947]\n",
      "../../multi-task-romain/2e_analyse/multitask/multi_model_15min_r2_97_94.pth\n",
      "iteration :  12\n",
      "\tTrain loss: 13.583 19.787\n",
      "\tValid loss: 37.636 \n",
      " valid r2 hr 0.967[0.964-0.969] valid r2 map 0.944[0.940-0.948]\n",
      "../../multi-task-romain/2e_analyse/multitask/multi_model_15min_r2_97_94.pth\n",
      "iteration :  13\n",
      "\tTrain loss: 13.370 19.400\n",
      "\tValid loss: 36.603 \n",
      " valid r2 hr 0.968[0.966-0.970] valid r2 map 0.945[0.941-0.949]\n",
      "../../multi-task-romain/2e_analyse/multitask/multi_model_15min_r2_97_95.pth\n",
      "iteration :  14\n",
      "\tTrain loss: 13.175 19.202\n",
      "\tValid loss: 36.102 \n",
      " valid r2 hr 0.969[0.967-0.971] valid r2 map 0.946[0.942-0.949]\n",
      "../../multi-task-romain/2e_analyse/multitask/multi_model_15min_r2_97_95.pth\n",
      "time consuming:  405.6943339999999 secs\n"
     ]
    }
   ],
   "source": [
    "model = EventModel1()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.05, weight_decay=1e-5)\n",
    "train_epochs(model, train_ds, optimizer, filename=\"multi_model\", epochs=10)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.03, weight_decay=1e-5)\n",
    "train_epochs(model, train_ds, optimizer,filename=\"multi_model\", epochs=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calibration plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../multi-task-romain/2e_analyse/multitask/multi_model_15min_r2_97_93.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-a508de1026fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"../../multi-task-romain/2e_analyse/multitask/multi_model_15min_r2_97_93.pth\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEventModel1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-20-4fce61fa246c>\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(m, p)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    523\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 525\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    526\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_reader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../multi-task-romain/2e_analyse/multitask/multi_model_15min_r2_97_93.pth'"
     ]
    }
   ],
   "source": [
    "file = \"../../multi-task-romain/2e_analyse/multitask/multi_model_15min_r2_97_95.pth\"\n",
    "model = EventModel1()\n",
    "load_model(model, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"data_test_{gap}.pickle\".format(gap=gap)\n",
    "with open(PATH/filename, 'rb') as f:\n",
    "    test = pickle.load(f)\n",
    "    \n",
    "filename = \"data_validation_{gap}.pickle\".format(gap=gap)\n",
    "with open(PATH/filename, 'rb') as f:\n",
    "    test_larib = pickle.load(f)\n",
    "test_larib[\"care_unit\"] = 4\n",
    "test.shape, test_larib.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_one_batch(model, dl):\n",
    "    for x_series, x_cont, x_cat, y1, y2 in dl:\n",
    "        x_series = x_series.float()\n",
    "        x_cont = x_cont.float()\n",
    "        x_cat = x_cat.long()\n",
    "        y1 = y1.float()\n",
    "        y2 = y2.float()\n",
    "        out1, out2 = model(x_series, x_cont, x_cat)\n",
    "    return out1.detach().numpy(), out2.detach().numpy(), y1.detach().numpy(), y2.detach().numpy()\n",
    "\n",
    "class MultiTask_validation(Dataset):\n",
    "    def __init__(self, df, norm_dict, id2index, k=20, train=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            df: dataframe with data\n",
    "            norm_dict: mean and std of all variables to normalize\n",
    "            \n",
    "        \"\"\"\n",
    "        self.norm_dict = norm_dict\n",
    "        self.df = df\n",
    "        self.names = [\"age\", \"sapsii\", \"sofa\"] ## needs normalization\n",
    "        self.names_binary = [\"gender\", \"amine\", \"sedation\", \"ventilation\"]\n",
    "        self.id2index = id2index\n",
    "        self.train = train\n",
    "        self.df_sample = self.pick_a_sample(k)\n",
    "            \n",
    "    def pick_a_sample(self, k=20):\n",
    "        \"\"\" Picks sample with the same number of observations per patient\"\"\"\n",
    "        if not self.train: # fix seed for validation and test\n",
    "            np.random.seed(3)\n",
    "# We don't want the same number of period per patient\n",
    "        # sample = self.df.groupby(\"subject_id\", group_keys=False).apply(lambda x: x.sample(k, replace=True))\n",
    "        sample = self.df.copy()\n",
    "        if self.train:\n",
    "# 10 percent of the periods have a subject_index == 0\n",
    "            self.subject_index = [self.id2index[subject_id] for subject_id in sample.subject_id.values]\n",
    "            self.random = np.random.choice(2, sample.shape[0], p = [0.1, 0.9])\n",
    "            self.subject_index = self.subject_index*self.random\n",
    "        return sample\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.df_sample.iloc[index,:] \n",
    "        x_series = (row.series - self.norm_dict[\"series\"][0])/self.norm_dict[\"series\"][1]\n",
    "        x_cont = [(row[name]-self.norm_dict[name][0])/self.norm_dict[name][1] for name in self.names]\n",
    "        x_binary = [row[name] for name in self.names_binary]\n",
    "        subject_index = 0\n",
    "        if self.train:\n",
    "            subject_index = self.subject_index[index]\n",
    "        x_cat = np.array([row[\"care_unit\"], subject_index])\n",
    "        x_cont = np.array(x_cont + x_binary)\n",
    "        return x_series, x_cont, x_cat, row[\"prediction_mean_HR\"], row[\"prediction_mean_MAP\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df_sample.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = MultiTask_validation(test, norm_dict, id2index, train = False)\n",
    "test_dl = DataLoader(test_ds, batch_size=8233)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#norm_dict_valid = get_mean_std_static(test_larib)\n",
    "test_larib_ds = MultiTask_validation(test_larib, norm_dict, id2index, train = False)\n",
    "# test_larib_ds = MultiTask(test_larib, norm_dict, id2index, train = False)\n",
    "test_larib_dl = DataLoader(test_larib_ds, batch_size=1597)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_metrics(model, test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_metrics(model, test_larib_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mimic III testing set predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out1, out2, y1, y2 = predict_one_batch(model, test_dl)\n",
    "y1 = np.reshape(y1, (-1,1))\n",
    "y2 = np.reshape(y2, (-1,1))\n",
    "arr_hr = np.concatenate((out1, y1) , axis=1)\n",
    "arr_map = np.concatenate((out2, y2) , axis=1)\n",
    "pd.DataFrame(arr_hr).to_csv(\"/home/menyssa/Recherche/Mimic-III-Yannet/resultats/2e_analyse/intern_obs_pred_HR_15.csv\")\n",
    "pd.DataFrame(arr_map).to_csv(\"/home/menyssa/Recherche/Mimic-III-Yannet/resultats/2e_analyse/intern_obs_pred_MAP_15.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(50, 160, 1000)\n",
    "plt.plot(out1, y1, 'bo', ms=5)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"mean_HR on test\")\n",
    "plt.plot(x, x, '-g')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(40, 140, 1000)\n",
    "plt.plot(out2, y2, 'bo', ms=5)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"mean_MAP on test\")\n",
    "plt.plot(x, x, '-g')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lariboisiere predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out1, out2, y1, y2 = predict_one_batch(model, test_larib_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, 200, 1000)\n",
    "plt.plot(out1, y1, 'bo', ms=5)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"mean_HR on larib\")\n",
    "plt.plot(x, x, '-g')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(40, 200, 1000)\n",
    "plt.plot(out2, y2, 'bo', ms=5)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"mean_MAP on larib\")\n",
    "plt.plot(x, x, '-g')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1 = np.reshape(y1, (-1,1))\n",
    "y2 = np.reshape(y2, (-1,1))\n",
    "arr_hr = np.concatenate((out1, y1) , axis=1)\n",
    "arr_map = np.concatenate((out2, y2) , axis=1)\n",
    "periode = np.reshape(test_larib.key.values, (-1,1))\n",
    "arr_hr = np.concatenate((out1, y1, periode) , axis=1)\n",
    "arr_map = np.concatenate((out2, y2, periode) , axis=1)\n",
    "pd.DataFrame(arr_hr).to_csv(\"/home/menyssa/Recherche/Mimic-III-Yannet/resultats/2e_analyse/larib_obs_pred_HR_15.csv\")\n",
    "pd.DataFrame(arr_map).to_csv(\"/home/menyssa/Recherche/Mimic-III-Yannet/resultats/2e_analyse/larib_obs_pred_MAP_15.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
